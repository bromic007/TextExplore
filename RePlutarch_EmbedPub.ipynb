{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RePlutarch_EmbedPub.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "owf8WA70lu5N"
      },
      "source": [
        "Copyright 2019 Almintas Povilaitis\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLlzaWcpUvU0",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td>\n",
        "<a target=\"_blank\"  href=\"https://colab.research.google.com/github/mlai-demo/TextExplore/blob/master/RePlutarch_EmbedPub.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "</td><td>\n",
        "<a target=\"_blank\"  href=\"https://github.com/mlai-demo/TextExplore/blob/master/RePlutarch_EmbedPub.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qjXCP6zHlu5Q"
      },
      "source": [
        "## The basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S0WezGZBlu5S"
      },
      "source": [
        "### Download the libraries and dataset\n",
        "\n",
        "Check the current directory and upload the text file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vtqYl3Zqlu5T",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "fpath = os.getcwd(); fpath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xNVq-Utllu5e",
        "colab": {}
      },
      "source": [
        "# if using Google Colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Click Files tab - the updloaded file(s) will be there"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9rIEuTW2lu5w"
      },
      "source": [
        "### Pre-process the text\n",
        "Tokenize, convert to lower case, remove some punctuation while preserving the sentence structure, then save the new text for future reference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xGq686PRsvsN",
        "colab": {}
      },
      "source": [
        "with open(fpath + '/Plutarch.txt') as f, open(fpath + '/Plutarch2.txt', 'w') as out_f:\n",
        "    text = f.read().lower()\n",
        "    new_text = re.sub('[^a-z\\.\\?\\!\\-\\'\\:\\;]', ' ', text) #keep only wanted characters (alphabet and select punctuation)    new_text = re.sub(' +', ' ', new_text)#remove double empty spaces between words\n",
        "    new_text = re.sub(' +', ' ', new_text) #remove double space\n",
        "    new_text = re.sub('\\n', ' ', new_text) #remove new line\n",
        "    items = [w for w in new_text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "    unique_items = set(items)\n",
        "    print(\"The text is {} words long, has {} unique items and {} characters on average\\n\".format\n",
        "      (len(items), len(unique_items), round(sum(len(word) for word in items)/len(items),2)))\n",
        "    print(\"First 1000 characters of the text:\\n\", new_text[:1000])\n",
        "    out_f.write(new_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9osObZcTlu6h"
      },
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fxp8QpfxmrD",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #used in Colab\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "data = []\n",
        "for i in sent_tokenize(new_text): \n",
        "    temp = [] \n",
        "      \n",
        "    # tokenize the sentence into words \n",
        "    for w in word_tokenize(i): \n",
        "        temp.append(w) \n",
        "            \n",
        "    data.append(temp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NcrO6TCExssv",
        "colab": {}
      },
      "source": [
        "#uncomment models you'd like to run\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "model_cbow = Word2Vec(data, min_count = 1,  size = 100, window = 8, iter=5, sg=0, hs=0) \n",
        "#model_sgram = Word2Vec(data, min_count = 1,  size = 100, window = 8, iter=5, sg=1, hs=0) \n",
        "#model_cbow_hs = Word2Vec(data, min_count = 1,  size = 100, window = 8, iter=5, sg=0, hs=1) \n",
        "#model_sgram_hs = Word2Vec(data, min_count = 1,  size = 100, window = 8, iter=5, sg=1, hs=1) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCAwYCvqwrC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Cosine similarity between 'caesar' \" + \"and 'king' - CBOW : \", \n",
        "    round(model_cbow.wv.similarity('caesar', 'king'),4)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn9oXjigwrgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similar_words = {search_term: [item[0] for item in model_cbow.wv.most_similar([search_term], topn=5)]\n",
        "                  for search_term in ['caesar', 'god', 'rome', 'greece', 'alexander', \n",
        "                                      'gaul', 'truth', 'king', 'hundred', 'sparta']}\n",
        "similar_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIlgIathwrvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vector of one word\n",
        "caesar = model_cbow.wv['caesar']; caesar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4O7qLeFRUwb",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOkOXjBdRScP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "vocab = list(model_cbow.wv.vocab)\n",
        "X = model_cbow.wv[vocab]\n",
        "pca2 = PCA(n_components=2) #number of dimensions\n",
        "X_pca2 = pca2.fit_transform(X)\n",
        "#df_pca2 = pd.DataFrame(X_pca2, index=vocab, columns=['x', 'y'])\n",
        "df_pca2 = pd.DataFrame(X_pca2, columns=['x', 'y'])\n",
        "df_pca2['word'] = vocab\n",
        "df_pca2 = df_pca2[['word','x','y']]\n",
        "df_pca2.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iab8RI0sPuks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "#vocab = list(model_cbow.wv.vocab)\n",
        "#X = model_cbow.wv[vocab]\n",
        "tsne2 = TSNE(n_components=2, random_state=0, n_iter=4000, perplexity=30) \n",
        "#tsne2 = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=55) #verbose=1?\n",
        "X_tsne2 = tsne2.fit_transform(X)\n",
        "#df_tsne22 = pd.DataFrame(X_tsne2, index=vocab, columns=['x', 'y'])\n",
        "df_tsne2 = pd.DataFrame(X_tsne2, columns=['x', 'y'])\n",
        "df_tsne2['word'] = vocab\n",
        "df_tsne2 = df_tsne2[['word','x', 'y']]\n",
        "df_tsne2.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr2313a_VtdX",
        "colab_type": "text"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6alCmP5EVxB-",
        "colab_type": "text"
      },
      "source": [
        "### Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHcpqHs0Pt2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import figure\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "fig = matplotlib.pyplot.gcf()\n",
        "fig.set_size_inches(18, 14)\n",
        "\n",
        "simwords = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "wvs = model_cbow.wv[simwords]\n",
        "\n",
        "pca_wvs = PCA(n_components=2, random_state=0)\n",
        "np.set_printoptions(suppress=True)\n",
        "Tpca = pca_wvs.fit_transform(wvs)\n",
        "labels = simwords\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.scatter(Tpca[:, 0], Tpca[:, 1], c='purple', edgecolors='purple')\n",
        "for label, x, y in zip(labels, Tpca[:, 0], Tpca[:, 1]):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY9Fyo8_PtO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import figure\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "fig = matplotlib.pyplot.gcf()\n",
        "fig.set_size_inches(18, 14)\n",
        "\n",
        "simwords = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "wvs = model_cbow.wv[simwords]\n",
        "\n",
        "tsne_wvs = TSNE(n_components=2, random_state=0, n_iter=4000, perplexity=30) \n",
        "np.set_printoptions(suppress=True)\n",
        "Ttsne = tsne_wvs.fit_transform(wvs)\n",
        "labels = simwords\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "plt.scatter(Ttsne[:, 0], Ttsne[:, 1], c='purple', edgecolors='purple')\n",
        "for label, x, y in zip(labels, Ttsne[:, 0], Ttsne[:, 1]):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFdaC-OhV2WH",
        "colab_type": "text"
      },
      "source": [
        "### Tensorflow projector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F3hVrAXUvVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a pandas dataframe out of the word2vec model (CBOW in this case)\n",
        "import pandas as pd\n",
        "project_wvs = [(term, voc.index, voc.count) for term, voc in model_cbow.wv.vocab.items()]\n",
        "project_wvs = sorted(project_wvs, key=lambda k: k[2])\n",
        "ordered_terms, term_indices, term_counts = zip(*project_wvs)\n",
        "df_cbow100 = pd.DataFrame(model_cbow.wv.vectors[term_indices, :], index=ordered_terms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fgafCHSRUvV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cbow100[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JxfHcqRUvV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_cbow100.to_csv('df_cbow100.tsv', sep='\\t', encoding='utf-8', index=True) #only if want to download labels and vectors in one file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "636Ygca0UvV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cbow100['word'] = df_cbow100.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9D2Sf7IUvWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cbow100['word'].to_csv('df_cbow100word.tsv', sep='\\t', encoding='utf-8', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEpYDRc-UvWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cbow100vector = df_cbow100.iloc[:,0:100].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pijQSjyDUvWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cbow100vector.to_csv('df_cbow100vector.tsv', sep='\\t', encoding='utf-8', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN1iuiluhK6B",
        "colab_type": "text"
      },
      "source": [
        "#### Upload the tsv files to Tensorflow Projector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-zDAzRLhec8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}